{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb4163e",
   "metadata": {},
   "source": [
    "## **Task  1 - Práctica**\n",
    "Para esta parte estarán resolviendo el problema de CartPole con Deep Q-Learning y una red de destino. Para esto, el \n",
    "objetivo de este ejercicio es entrenar a un agente para que equilibre un poste en un carro (cartpole) en movimiento \n",
    "durante el mayor tiempo posible. Se deberá usar Deep Q-Learning (DQL) con una red objetivo para lograr esto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d823d23",
   "metadata": {},
   "source": [
    "#### 1. librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda8e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4f055",
   "metadata": {},
   "source": [
    "#### 2. Entorno en Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f0d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593aae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=123)\n",
    "obs_dim   = env.observation_space.shape[0]\n",
    "action_dim= env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181d123",
   "metadata": {},
   "source": [
    "### 3. Definan las redes en línea y de destino:\n",
    "\n",
    "Cree dos redes neuronales, la red en línea y la red de destino. La red \n",
    "en línea se utiliza para la selección de acciones y se actualiza con más frecuencia, mientras que la red de \n",
    "destino se utiliza para estimar los valores Q y se actualiza periódicamente. Ambas redes deberían tener una \n",
    "arquitectura similar con capas de entrada y salida. Inicialmente, la red de destino debería tener los mismos \n",
    "pesos que la red en línea.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15926c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden=(128, 128)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden[1], action_dim)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "online_q = QNetwork(obs_dim, action_dim).to(device)\n",
    "target_q = QNetwork(obs_dim, action_dim).to(device)\n",
    "\n",
    "# target inicial \n",
    "target_q.load_state_dict(online_q.state_dict())\n",
    "for p in target_q.parameters():\n",
    "    p.requires_grad = False\n",
    "target_q.eval()\n",
    "\n",
    "# Optimizador y loss\n",
    "optimizer = optim.Adam(online_q.parameters(), lr=1e-3)\n",
    "loss_fn   = nn.SmoothL1Loss()\n",
    "\n",
    "# Sincronizacion de redes\n",
    "def hard_update(target: nn.Module, source: nn.Module):\n",
    "    target.load_state_dict(source.state_dict())\n",
    "\n",
    "@torch.no_grad()\n",
    "def soft_update(target: nn.Module, source: nn.Module, tau: float = 0.005):\n",
    "    for tp, sp in zip(target.parameters(), source.parameters()):\n",
    "        tp.data.mul_(1.0 - tau).add_(sp.data, alpha=tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b7fa4",
   "metadata": {},
   "source": [
    "#### 4. Establecer hiperparámetros\n",
    "Defina hiperparámetros como el número de episodios, el tamaño de los \n",
    "batches, el factor de descuento (gamma) y los parámetros de exploración (epsilon, epsilon decay, epsilon \n",
    "mínimo). Ajuste estos hiperparámetros según sea necesario para optimizar el entrenamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aae237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DQNConfig:\n",
    "    episodes: int = 500\n",
    "    buffer_capacity: int = 50_000\n",
    "    warmup_steps: int = 5_000\n",
    "    batch_size: int = 64\n",
    "    gamma: float = 0.99\n",
    "    lr: float = 1e-3\n",
    "\n",
    "    # Exploracion\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_end: float = 0.03\n",
    "    epsilon_decay_steps: int = 15_000\n",
    "    use_exponential_eps: bool = False\n",
    "\n",
    "    # Target network\n",
    "    target_update_every: int = 1_000\n",
    "    use_soft_update: bool = False\n",
    "    tau: float = 0.005\n",
    "\n",
    "    grad_clip: float = 10.0\n",
    "    seed: int = 123\n",
    "\n",
    "cfg = DQNConfig()\n",
    "\n",
    "def epsilon_linear(step: int, cfg: DQNConfig):\n",
    "    frac = min(1.0, step / cfg.epsilon_decay_steps)\n",
    "    return cfg.epsilon_start + (cfg.epsilon_end - cfg.epsilon_start) * frac\n",
    "\n",
    "def epsilon_exponential(step: int, cfg: DQNConfig):\n",
    "    decay = (cfg.epsilon_end / cfg.epsilon_start) ** (1.0 / cfg.epsilon_decay_steps)\n",
    "    return max(cfg.epsilon_end, cfg.epsilon_start * (decay ** step))\n",
    "\n",
    "def get_epsilon(global_step: int, cfg: DQNConfig):\n",
    "    if cfg.use_exponential_eps:\n",
    "        return epsilon_exponential(global_step, cfg)\n",
    "    return epsilon_linear(global_step, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53de351",
   "metadata": {},
   "source": [
    "#### 5. selección de acciones épsilon-greedy\n",
    "Cree una función para la selección de acciones épsilon greedy. Esta función ayuda al agente a elegir acciones basadas en la política épsilon-greedy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def select_action(state_np: np.ndarray, online_q: nn.Module, action_dim: int, device: torch.device, epsilon: float):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(action_dim)\n",
    "    s = torch.tensor(state_np, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    q = online_q(s)\n",
    "    return int(q.argmax(dim=1).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8df70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acciones (a, eps) en 10 pasos de prueba: [(0, 1.0), (1, 1.0), (0, 1.0), (1, 1.0), (1, 1.0), (0, 1.0), (0, 1.0), (1, 1.0), (1, 0.999), (0, 0.999)]\n"
     ]
    }
   ],
   "source": [
    "# prueba para verrificar que funcione, no hace entrenamiento\n",
    "global_step = 0\n",
    "state, _ = env.reset(seed=cfg.seed)\n",
    "done = False\n",
    "taken = []\n",
    "for _ in range(10): \n",
    "    eps = get_epsilon(global_step, cfg)\n",
    "    a = select_action(state, online_q, action_dim, device, eps)\n",
    "    taken.append((a, round(eps, 3)))\n",
    "    state, reward, terminated, truncated, _ = env.step(a)\n",
    "    done = terminated or truncated\n",
    "    global_step += 1\n",
    "    if done:\n",
    "        state, _ = env.reset(seed=cfg.seed)\n",
    "\n",
    "print(\"Acciones (a, eps) en 10 pasos de prueba:\", taken)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2848d48",
   "metadata": {},
   "source": [
    "## **Práctica 2 - Teoría**\n",
    "\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc451e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b959cd1",
   "metadata": {},
   "source": [
    "### 1. Proximal Policy Optimization (PPO)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- PPO es un método **on-policy** de optimización de políticas.  \n",
    "- Busca un balance entre **mejorar la recompensa esperada** y **mantener la política actual cercana a la anterior**.  \n",
    "- Introduce una **función de pérdida con un término de \"clipping\"** que evita actualizaciones muy grandes en los parámetros de la política, lo que ayuda a mantener la estabilidad.  \n",
    "- Generalmente se implementa como **actor-critic**: el actor propone acciones y el crítico estima su valor.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Simplicidad en la implementación.  \n",
    "- Estable y confiable.  \n",
    "- Menos costoso computacionalmente que TRPO, pero con resultados similares.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Juegos como **Atari**, **StarCraft II**, **Dota 2**.  \n",
    "- Entornos de **control continuo en robótica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698be6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f368212",
   "metadata": {},
   "source": [
    "### 2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- DDPG es un método **off-policy** y **actor-critic** para espacios de acción continuos.  \n",
    "- El **actor** produce acciones determinísticas (no probabilísticas).  \n",
    "- El **crítico** estima la función Q (valor esperado de una acción en un estado).  \n",
    "- Usa un **replay buffer** para almacenar experiencias pasadas y entrenar con ellas, reduciendo la correlación de datos.  \n",
    "- Utiliza **target networks suavizadas** para mejorar la estabilidad del entrenamiento.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Adecuado para problemas de **alto control continuo**.  \n",
    "- Puede aprender políticas muy precisas en tareas de control físico.  \n",
    "- Puede ser inestable si no se ajustan bien los hiperparámetros.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Control de robots (ej. un brazo robótico que debe agarrar objetos).  \n",
    "- Conducción autónoma.  \n",
    "- Tareas de locomoción en simuladores 3D (ej. MuJoCo).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a5bf02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75bce0",
   "metadata": {},
   "source": [
    "### 3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- TRPO es un algoritmo **on-policy** basado en políticas.  \n",
    "- En lugar de permitir actualizaciones arbitrarias de la política, **impone una restricción de confianza (trust region)**.  \n",
    "- Esta restricción se mide con la **KL-divergence**, asegurando que la política nueva no se desvíe demasiado de la anterior.  \n",
    "- Se enfoca en **estabilidad garantizada**, aunque a costa de más complejidad computacional.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Muy estable en comparación con otros métodos.  \n",
    "- Requiere resolver un problema de optimización más costoso.  \n",
    "- Sirvió de base para el desarrollo de PPO.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Entornos de **robótica realista**, donde la estabilidad es crítica.  \n",
    "- Tareas de control continuo donde un error drástico sería costoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd09fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe843b14",
   "metadata": {},
   "source": [
    "### 4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- A3C es un método **actor-critic** que entrena **múltiples agentes en paralelo** (cada uno con una copia del entorno).  \n",
    "- Cada agente actualiza de manera asíncrona los parámetros compartidos del modelo global.  \n",
    "- Usa la ventaja (**$Advantage = Q(s,a) – V(s)$**) para reducir la varianza en el entrenamiento.  \n",
    "- La paralelización acelera la exploración y evita caer en mínimos locales.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Eficiente en términos de exploración y velocidad.  \n",
    "- Reduce la correlación entre muestras al tener múltiples agentes explorando en paralelo.  \n",
    "- Puede ser más inestable que PPO en algunos casos.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Juegos de Atari.  \n",
    "- Entornos 3D complejos (ej. Doom, Minecraft).  \n",
    "- Simulaciones con muchos estados posibles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817b15",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd092a58",
   "metadata": {},
   "source": [
    "### Tabla comparativa\n",
    "\n",
    "| Algoritmo | Tipo | Idea principal | Ventajas | Limitaciones | Casos típicos |\n",
    "|-----------|------|----------------|----------|--------------|---------------|\n",
    "| **PPO** (Proximal Policy Optimization) | On-policy, Actor-Critic | Función de pérdida con *clipping* para limitar cambios bruscos en la política | Estable, eficiente, fácil de implementar | Menos muestra-eficiente que off-policy | Juegos (Atari, Dota2), robótica |\n",
    "| **DDPG** (Deep Deterministic Policy Gradients) | Off-policy, Actor-Critic | Actor produce acciones continuas determinísticas, crítico evalúa con función Q, usa replay buffer | Maneja acciones continuas, buena precisión | Sensible a hiperparámetros, menos estable | Robótica, control autónomo, locomoción |\n",
    "| **TRPO** (Trust Region Policy Optimization) | On-policy, Policy Gradient | Restringe la actualización de la política usando KL-divergence (trust region) | Muy estable, garantías teóricas | Computacionalmente costoso, difícil de implementar | Robótica avanzada, simulaciones físicas |\n",
    "| **A3C** (Asynchronous Advantage Actor-Critic) | On-policy, Actor-Critic asíncrono | Entrena múltiples agentes en paralelo, cada uno explora y actualiza parámetros globales | Explora más rápido, menos correlación, eficiente | Puede ser inestable, reemplazado por PPO en muchos casos | Atari, videojuegos 3D, exploración compleja |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e42c36",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- DhanushKumar. (2025, agosto 11). PPO algorithm. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "\n",
    "- Marekar, A. (2022, junio 12). How DDPG (Deep Deterministic Policy Gradient) Algorithms works in reinforcement learning ? Medium. https://medium.com/@amaresh.dm/how-ddpg-deep-deterministic-policy-gradient-algorithms-works-in-reinforcement-learning-117e6a932e68\n",
    "\n",
    "- Sciforce. (2021, marzo 25). Reinforcement learning and Asynchronous Actor-Critic Agent (A3C) Algorithm, explained. Medium. https://medium.com/sciforce/reinforcement-learning-and-asynchronous-actor-critic-agent-a3c-algorithm-explained-f0f3146a14ab\n",
    "\n",
    "- Wu, H. (2024, marzo 9). Trust Region Policy Optimization explained. Medium. https://medium.com/@hsinhungw/trust-region-policy-optimization-explained-c2671542c329"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
