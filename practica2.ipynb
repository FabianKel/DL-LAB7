{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2848d48",
   "metadata": {},
   "source": [
    "## **Práctica 2 - Teoría**\n",
    "\n",
    "Defina en qué consiste y en qué clase de problemas se pueden usar cada uno de los siguientes acercamientos en\n",
    "Deep Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc451e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b959cd1",
   "metadata": {},
   "source": [
    "### 1. Proximal Policy Optimization (PPO)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- PPO es un método **on-policy** de optimización de políticas.  \n",
    "- Busca un balance entre **mejorar la recompensa esperada** y **mantener la política actual cercana a la anterior**.  \n",
    "- Introduce una **función de pérdida con un término de \"clipping\"** que evita actualizaciones muy grandes en los parámetros de la política, lo que ayuda a mantener la estabilidad.  \n",
    "- Generalmente se implementa como **actor-critic**: el actor propone acciones y el crítico estima su valor.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Simplicidad en la implementación.  \n",
    "- Estable y confiable.  \n",
    "- Menos costoso computacionalmente que TRPO, pero con resultados similares.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Juegos como **Atari**, **StarCraft II**, **Dota 2**.  \n",
    "- Entornos de **control continuo en robótica**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698be6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f368212",
   "metadata": {},
   "source": [
    "### 2. Deep Deterministic Policy Gradients (DDPG)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- DDPG es un método **off-policy** y **actor-critic** para espacios de acción continuos.  \n",
    "- El **actor** produce acciones determinísticas (no probabilísticas).  \n",
    "- El **crítico** estima la función Q (valor esperado de una acción en un estado).  \n",
    "- Usa un **replay buffer** para almacenar experiencias pasadas y entrenar con ellas, reduciendo la correlación de datos.  \n",
    "- Utiliza **target networks suavizadas** para mejorar la estabilidad del entrenamiento.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Adecuado para problemas de **alto control continuo**.  \n",
    "- Puede aprender políticas muy precisas en tareas de control físico.  \n",
    "- Puede ser inestable si no se ajustan bien los hiperparámetros.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Control de robots (ej. un brazo robótico que debe agarrar objetos).  \n",
    "- Conducción autónoma.  \n",
    "- Tareas de locomoción en simuladores 3D (ej. MuJoCo).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a5bf02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75bce0",
   "metadata": {},
   "source": [
    "### 3. Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- TRPO es un algoritmo **on-policy** basado en políticas.  \n",
    "- En lugar de permitir actualizaciones arbitrarias de la política, **impone una restricción de confianza (trust region)**.  \n",
    "- Esta restricción se mide con la **KL-divergence**, asegurando que la política nueva no se desvíe demasiado de la anterior.  \n",
    "- Se enfoca en **estabilidad garantizada**, aunque a costa de más complejidad computacional.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Muy estable en comparación con otros métodos.  \n",
    "- Requiere resolver un problema de optimización más costoso.  \n",
    "- Sirvió de base para el desarrollo de PPO.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Entornos de **robótica realista**, donde la estabilidad es crítica.  \n",
    "- Tareas de control continuo donde un error drástico sería costoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd09fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe843b14",
   "metadata": {},
   "source": [
    "### 4. Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "**¿En qué consiste?**  \n",
    "- A3C es un método **actor-critic** que entrena **múltiples agentes en paralelo** (cada uno con una copia del entorno).  \n",
    "- Cada agente actualiza de manera asíncrona los parámetros compartidos del modelo global.  \n",
    "- Usa la ventaja (**$Advantage = Q(s,a) – V(s)$**) para reducir la varianza en el entrenamiento.  \n",
    "- La paralelización acelera la exploración y evita caer en mínimos locales.  \n",
    "\n",
    "**Características clave:**  \n",
    "- Eficiente en términos de exploración y velocidad.  \n",
    "- Reduce la correlación entre muestras al tener múltiples agentes explorando en paralelo.  \n",
    "- Puede ser más inestable que PPO en algunos casos.  \n",
    "\n",
    "**¿Dónde se usa?**  \n",
    "- Juegos de Atari.  \n",
    "- Entornos 3D complejos (ej. Doom, Minecraft).  \n",
    "- Simulaciones con muchos estados posibles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57817b15",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd092a58",
   "metadata": {},
   "source": [
    "### Tabla comparativa\n",
    "\n",
    "| Algoritmo | Tipo | Idea principal | Ventajas | Limitaciones | Casos típicos |\n",
    "|-----------|------|----------------|----------|--------------|---------------|\n",
    "| **PPO** (Proximal Policy Optimization) | On-policy, Actor-Critic | Función de pérdida con *clipping* para limitar cambios bruscos en la política | Estable, eficiente, fácil de implementar | Menos muestra-eficiente que off-policy | Juegos (Atari, Dota2), robótica |\n",
    "| **DDPG** (Deep Deterministic Policy Gradients) | Off-policy, Actor-Critic | Actor produce acciones continuas determinísticas, crítico evalúa con función Q, usa replay buffer | Maneja acciones continuas, buena precisión | Sensible a hiperparámetros, menos estable | Robótica, control autónomo, locomoción |\n",
    "| **TRPO** (Trust Region Policy Optimization) | On-policy, Policy Gradient | Restringe la actualización de la política usando KL-divergence (trust region) | Muy estable, garantías teóricas | Computacionalmente costoso, difícil de implementar | Robótica avanzada, simulaciones físicas |\n",
    "| **A3C** (Asynchronous Advantage Actor-Critic) | On-policy, Actor-Critic asíncrono | Entrena múltiples agentes en paralelo, cada uno explora y actualiza parámetros globales | Explora más rápido, menos correlación, eficiente | Puede ser inestable, reemplazado por PPO en muchos casos | Atari, videojuegos 3D, exploración compleja |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e42c36",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- DhanushKumar. (2025, agosto 11). PPO algorithm. Medium. https://medium.com/@danushidk507/ppo-algorithm-3b33195de14a\n",
    "\n",
    "- Marekar, A. (2022, junio 12). How DDPG (Deep Deterministic Policy Gradient) Algorithms works in reinforcement learning ? Medium. https://medium.com/@amaresh.dm/how-ddpg-deep-deterministic-policy-gradient-algorithms-works-in-reinforcement-learning-117e6a932e68\n",
    "\n",
    "- Sciforce. (2021, marzo 25). Reinforcement learning and Asynchronous Actor-Critic Agent (A3C) Algorithm, explained. Medium. https://medium.com/sciforce/reinforcement-learning-and-asynchronous-actor-critic-agent-a3c-algorithm-explained-f0f3146a14ab\n",
    "\n",
    "- Wu, H. (2024, marzo 9). Trust Region Policy Optimization explained. Medium. https://medium.com/@hsinhungw/trust-region-policy-optimization-explained-c2671542c329"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
